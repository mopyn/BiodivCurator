{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, glob, shutil\n",
    "import openpyxl\n",
    "import re # For regular expressions\n",
    "from LatLon23 import string2latlon\n",
    "import requests\n",
    "\n",
    "# Import own module\n",
    "from PyCurator.PyCurator import PyCurator as pc\n",
    "from PyCurator.PyCurator import PanData\n",
    "import importlib\n",
    "importlib.reload(pc)\n",
    "importlib.reload(PanData)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load parameter database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download PANGAEA parameters\n",
    "db = PanData.get_PanParameters(\"C:\\\\Users\\\\marum2022\\\\Documents\\\\MARUM\\\\Tickets\\\\Database\\\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup curation folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter author folder\n",
    "author = \"Spatafora_etal_PDI-33204\"\n",
    "# Store file path\n",
    "file_path = f'C:\\\\Users\\\\marum2022\\\\Documents\\\\MARUM\\Tickets\\\\Open tickets\\\\{author}\\\\'\n",
    "# Change the current working directory\n",
    "os.chdir(file_path)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create working folders and move data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create curation folders\n",
    "new_folders = [\"Dataset\", \"Dataset_curated\", \"Parameters\", \"Events\", \"Plots\"]\n",
    "\n",
    "for folder in new_folders:\n",
    "    # Check if folder already exists\n",
    "    if os.path.isdir(folder):\n",
    "        print(f'{folder} already exists')\n",
    "        continue\n",
    "    # if not create it       \n",
    "    else:\n",
    "        os.mkdir(folder)\n",
    "\n",
    "# And move all data files into Dataset folder\n",
    "[shutil.copy(file, f'{file_path}Dataset\\\\') for file in pc.list_files(file_path)]\n",
    "# Remove files\n",
    "[os.remove(file) for file in pc.list_files(file_path)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read user files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read user files\n",
    "df_dict = pc.load_df(sub_folder= file_path + \"Dataset\\\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General information about dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General information for all data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe names\n",
    "df_names = df_dict.keys()\n",
    "print(df_names)\n",
    "# General information for all data files\n",
    "for df_name, df in df_dict.items():\n",
    "    print(\"-\"*70)\n",
    "    print(f'The dataset {df_name} has {df.shape[0]} rows and {df.shape[1]} columns')\n",
    "    print(pd.concat([df.head(2), df.tail(2)]).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual file processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty dictionary to store curated data frames\n",
    "df_curated = {}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 1: Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does the dataset look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all file names\n",
    "df_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset name\n",
    "df_name = 'Dataset'\n",
    "# Create a copy of dataframe\n",
    "df = df_dict[df_name].copy()\n",
    "print(f'The dataframe {df_name} has {df.shape[0]} rows and {df.shape[1]} columns')\n",
    "pd.concat([df.head(3), df.tail(3)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove empty columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First delete any empty columns\n",
    "df.dropna(how='all', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General overview and data types\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace user with database parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print column labels line by line with added [#]\n",
    "for x in df.columns:\n",
    "    print(f\"'{x} []',\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find close matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Search for close matches in PANGAEA database for single string\n",
    "# search_term = \"Distance from shelter []\"\n",
    "# PanData.get_close_match([search_term], db[\"Parameter\"] + \" [\" + db[\"Unit\"] + \"]\", n_matches = 15).T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find start search term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find all terms starting with search term\n",
    "# search_term = \"Distance from shelter\"\n",
    "# str_start = db[\"Parameter\"][db[\"Parameter\"].str.lower().str.contains(search_term.lower(), )]\n",
    "# str_start\n",
    "# # Subfilter results with further match\n",
    "# str_start[str_start.str.contains(\"\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New events to be replaced\n",
    "new_params = [\n",
    "            'Class',\n",
    "            'Order',\n",
    "            'Family',\n",
    "            'Species',\n",
    "            'Species, common name',\n",
    "            'IUCN status',\n",
    "            'Life stage',\n",
    "            'DATE/TIME',\n",
    "            'Month',\n",
    "            'Year',\n",
    "            'Location',\n",
    "            'LATITUDE',\n",
    "            'LONGITUDE',\n",
    "            'Photographer',\n",
    "            'Comment'\n",
    "            ]\n",
    "# Use simple replace method\n",
    "df.columns = new_params\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete all leading and trailing white space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all leading and trailing white spaces \n",
    "cols = df.select_dtypes(['object']).columns\n",
    "\n",
    "# Remove all leading and trailing white spaces \n",
    "for col in cols:\n",
    "    # Remove leading and trailing white space\n",
    "    df[col].replace(r\"^ +| +$\", r\"\", regex=True, inplace=True)\n",
    "    # Remove double white spaces\n",
    "    df[col].replace(r\"\\s+\", r\" \", regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to PANGAEA standard date format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PANGAEA date format\n",
    "df[\"DATE/TIME []\"] = pc.toPangaeaDate(df[\"DATE/TIME []\"], '%Y-%m-%d')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spell out abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"abbreviation\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list with abbreviations\n",
    "short = (\"NE\",\n",
    "              \"DD\",\n",
    "              \"LC\",\n",
    ")\n",
    "\n",
    "# Create list with full names\n",
    "long = (\"Not Evaluated\", \n",
    "              \"Data Deficient\", \n",
    "              \"Least Concern\", \n",
    ")\n",
    "\n",
    "# Create dictionary from long and short names\n",
    "labels = dict(zip(short, long))\n",
    "\n",
    "# Replace the values in 'Name' column with the dictionary\n",
    "df = df.replace({\"abbreviation\": labels})\n",
    "\n",
    "df[\"abbreviation\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for other strings in numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.count_char(df, \"[a-zA-Z]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert all commas to dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pc.replace_char(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert object to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert multiple columns of dataframe to numeric\n",
    "df.iloc[:, 2:] = df.iloc[:, 2:].apply(pd.to_numeric)\n",
    "#Check numeric features for text entries\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add methods details for species only\n",
    "df.columns.values[2:-3] = df.columns.values[2:-3] + \"//*counted from 10 samples\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add event label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert event column at first position\n",
    "df.insert(0,'Event []', \"EN18224-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find matching parameters in database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find unmatched parameters in database and create import table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find matched and unmatched items of one dataframe in another\n",
    "_, unmatched = PanData.find_df_matches(PanData.create_find_df(PanData.strip_comments(df.columns)), db)\n",
    "unmatched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close_matches = PanData.close_db_matches(unmatched, db, n_matches=5)\n",
    "# close_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find unmatched parameters in database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new data table without comments in header\n",
    "df2 = df.copy()\n",
    "df2.columns = PanData.strip_comments(df2.columns)\n",
    "\n",
    "# Get unmatched dataframe entries\n",
    "unmatched_param_df = PanData.get_unmatched_df(df2, db)\n",
    "unmatched_param_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create import table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find unmatched parameters in database and create import table\n",
    "imp_table = PanData.get_imp_param(unmatched_param_df,\n",
    "                        file_path = f'{file_path}Parameters\\\\',\n",
    "                        DefaultMethodID = \"\", \n",
    "                        ReferenceID = \"\", \n",
    "                        Description = \"\", \n",
    "                        url_parameter = \"\",\n",
    "                        df_name = df_name,\n",
    "                        dec_limit = 4)\n",
    "imp_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store curated dataframe in dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_curated[df_name] = df\n",
    "print(f'Dataset: {df_name}')\n",
    "df_curated[df_name].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign unique species names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save unique species names for taxon matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Species []\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new species folder\n",
    "folder = \"Species_naming\"\n",
    "# Check if folder already exsits\n",
    "if not os.path.isdir(folder):\n",
    "    os.mkdir(folder)\n",
    "# Save species names\n",
    "species = df[\"Stomach content []//*found in Gadus morhua//$11775\"].drop_duplicates().to_frame()\n",
    "# Rename header\n",
    "species.columns = [\"name\"]\n",
    "# Save as csv file\n",
    "species\n",
    "species.to_csv(f'{file_path}{folder}\\\\{df_name}_Species.txt', index=False, sep=\"\\t\", encoding='ansi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load ITIS match list\n",
    "Saved species list will be matched in the taxon match tools of:\n",
    "- [WoRMS](https://www.marinespecies.org/aphia.php?p=match) \n",
    "- [ITIS](https://www.itis.gov/taxmatch.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load matched taxon file\n",
    "itis = pd.read_csv(f'{file_path}{folder}\\\\{df_name}_Species_ITIS.csv', sep=\"|\", encoding = \"ISO-8859-1\")\n",
    "# Convert TSN to integer\n",
    "itis.TSN = itis.TSN.astype('Int64')\n",
    "itis.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Match TSN to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates from itis dataframe\n",
    "itis = itis.drop_duplicates(subset=\"Scientific Name\")\n",
    "# Merge two dataframes on two common columns\n",
    "df = pd.merge(df, itis.iloc[:,[0, 1]],\n",
    "    how=\"left\",\n",
    "    left_on=\"Species []\",\n",
    "    right_on= \"Scientific Name\"\n",
    ").drop(\"Scientific Name\", axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Species (Semantic URI) columns from  TSN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create semantic URI by pasting urn and TSN together\n",
    "df.insert(4, \"Species, unique identification []\", [f'urn:lsid:itis.gov:itis_tsn:{str(x)}' for x in df.TSN])\n",
    "# Remove TSN column\n",
    "df = df.drop([\"TSN\"], axis = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new events based on unique latitude and longitude"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First generate events based on unique lat/long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First extract Location and lat and long\n",
    "events = {}\n",
    "for key, df in df_curated.items():\n",
    "    events[key] = df.loc[:, ['Location []', '1600', '1601']]\n",
    "# Then concatenate all data  \n",
    "events = pd.concat(events, ignore_index = True)\n",
    "\n",
    "# Create new events based on unique latitude and longitude\n",
    "# Define target columns\n",
    "cols = ['1600', '1601']\n",
    "# Sort dataframe\n",
    "events = events.sort_values(by=cols, ascending=False)\n",
    "# Drop duplicates\n",
    "events = events.drop_duplicates(subset = cols, keep = \"first\")\n",
    "# Create event name\n",
    "events.insert(0, \"Event []\", [f'CO2_Behav_2022_{x}' for x in range(1,len(events)+1)])\n",
    "events                                    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then merge those with data frames based on lat/lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge an event column to all dataframes in a dictionary by matching with lat and long\n",
    "df_new = {}\n",
    "for key, df in df_curated.items():\n",
    "    df_new[key] = df.loc[:, ['1600', '1601']]\n",
    "    # Merge events with original dataframe\n",
    "    df = pd.merge(df, events.loc[:, ['Event []', '1600', '1601']],\n",
    "        how=\"left\",\n",
    "        on=['1600', '1601'],\n",
    "    )\n",
    "    # Move Event column to first column\n",
    "    df.insert(0, 'Event []', df.pop(\"Event []\"))\n",
    "    # Store data with event column in dictionary\n",
    "    df_new[key] = df\n",
    "    #print(df_new[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final data check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable descriptive stats of multiple files\n",
    "for key, df in df_curated.items():\n",
    "    print(\"-\"*70)\n",
    "    print(f'The dataset {key} has {df.shape[0]} rows and {df.shape[1]} columns')\n",
    "    print(f'Data types:\\n{df.dtypes.value_counts()}')\n",
    "    print(df.head(2).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable descriptive stats of multiple files\n",
    "for key,  df in df_curated.items():\n",
    "    if (df.dtypes != \"object\").any():\n",
    "        print(\"-\"*70)\n",
    "        print(f'Dataset: {key}')\n",
    "        print(round(df.describe().loc[[\"mean\", \"min\", \"max\"]], 2).T)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot all numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all numeric features for each dataframe\n",
    "for key,  df in df_curated.items():\n",
    "    # Lineplot for each numeric feature\n",
    "    pc.plot_data(df, plot_name=key, file_path=f'{file_path}Plots')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary for all non-numeric data types\n",
    "for key, df in df_curated.items():\n",
    "    print(\"-\"*70)\n",
    "    print(f'The dataset {key} has {df.shape[0]} rows and {df.shape[1]} columns')\n",
    "    if (df.dtypes == \"object\").any():\n",
    "        print(df.describe(exclude=[np.number]).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join new parameter tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read metadata files\n",
    "param_dict = pc.load_df(sub_folder= f'{file_path}Parameters\\\\')\n",
    "\n",
    "# Join all dataframes in the dictionary\n",
    "param_imp = pd.concat(param_dict.values(), ignore_index=True)\n",
    "\n",
    "# Remove duplicate rows\n",
    "param_imp = param_imp.drop_duplicates(subset=\"ParameterName\", ignore_index=True)\n",
    "\n",
    "# Save final parameter import table\n",
    "param_imp.to_csv(f'{file_path}Parameters\\\\ParamImp_{author}_MOellermann.txt', index=False, sep=\"\\t\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_curated.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save curated files\n",
    "for key, df in df_curated.items():\n",
    "    # Create subfolder path\n",
    "    sub_folder = f'{file_path}Dataset_curated\\\\{key}\\\\'\n",
    "    # Check if folder already exists\n",
    "    if not os.path.isdir(sub_folder):\n",
    "        # if not create it    \n",
    "        os.mkdir(sub_folder)\n",
    "\n",
    "    # Save file\n",
    "    # as tab delimited csv file\n",
    "    df.to_csv(f'{sub_folder}{key}_curated.txt', index=False, sep=\"\\t\", encoding='ansi')\n",
    "    # as excel\n",
    "    if df.shape[0] < 1048576: # Checks maximum allowed sheet size for excel\n",
    "        df.to_excel(f'{sub_folder}{key}_curated.xlsx', index=False)\n",
    "    print(f'Dataset {key} saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First extract event details\n",
    "events = df[['Event []', '1600', '1601', 'Site []']]\n",
    "# Save as csv\n",
    "events.to_csv(f'{file_path}Events//MassImport_Event.txt', index=False, sep=\"\\t\", encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ab424e4dc2e74cacaae9c1f914107a9f781b476e0d613ee58688e62f3501b19d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
